[1장](https://velog.io/@qjsmdk1346/Book-%EB%9F%AC%EB%8B%9D-%EC%8A%A4%ED%8C%8C%ED%81%AC-1%EC%9E%A5-%EC%95%84%ED%8C%8C%EC%B9%98-%EC%8A%A4%ED%8C%8C%ED%81%AC-%EC%86%8C%EA%B0%9C-%ED%86%B5%ED%95%A9-%EB%B6%84%EC%84%9D-%EC%97%94%EC%A7%84)에 이어서 공부해보자.

# 2. 아파치 스파크 다운로드 및 시작

이 장에서는 스파크 셋업과 간단한 세 단계를 거쳐 첫 단독 애플리케이션을 작성할 수 있게 될 것이다.

## 2.1. 1단계: 아파치 스파크 다운로드

아파치 스파크 2.2 릴리스 이후 스파크를 파이썬으로 배우려는 개발자들에게는 PyPI 저장소로부터 파이스파크(PySpark)를 설치하는 방법이 있다.

PyPI로 파이스파크를 설치하려면 단순히 

```
pip install pyspark
```

만 입력하면 된다.

### 2.1.1. 스파크의 디렉터리와 파일들

## 2.2. 2단계: 스칼라 혹은 파이스파크 셀 사용

대화형 '셸'로 동작하며 광범위하게 사용되는 pyspark, spark-shell, spark-sql, sparkR의 네 가지 인터프리터들이 포함되어 있어서 일회성 데이터 분석이 가능하다.

## 2.3. 로컬 머신 사용하기

## 2.4. 3단계: 스파크 애플리케이션 개념의 이해

### 2.4.1. 스파크 애플리케이션과 SparkSession

모든 스파크 애플리케이션의 핵심에는 스파크 드라이버 프로그램이 있으며, 이 드라이버는 SparkSession 객체를 만든다.

스파크 셸을 써서 작업을 할 때 드라이버는 셸에 포함되어 있는 형태이며 이전 예제들에서 봤던 것처럼 SparkSession 객체(spark라는 이름의 변수로 접근 가능하다)가 미리 만들어진다.

### 2.4.2. 스파크 잡

스파크 셸로 상호 작용하는 작업 동안, 드라이버는 스파크 애플리케이션을 하나 이상의 스파크 잡으로 변환한다.

그리고 각 잡은 DAG로 변환된다. 본질적으로 이것이 스파크의 실행 계획이 되며 이 DAG 그래프에서 각각의 노드는 하나 이상의 스파크 스테이지에 해당한다.

### 2.4.3. 스파크 스테이지

어떤 작업이 연속적으로 또는 병렬적으로 수행되는지에 맞춰 스테이지에 해당하는 DAG 노드가 생성된다. 모든 스파크 연산이 하나의 스테이지 안에서 실행될 수는 없으므로 여러 스테이지로 나뉘어야 한다.

### 2.4.4. 스파크 태스크

각 스테이지는 최소 실행 단위이며 스파크 이그제큐터들 위에서 연합 실행되는 스파크 태스크들로 이루어진다. 각 태스크는 개별 CPU 코어에 할당되고 데이터의 개별 파티션을 갖고 작업한다. 그런 식으로, 16코어 이그제큐터라면 16개 이상의 파티션을 갖는 16개 이상의 태스크를 할당받아 작업하게 되며 이런 식으로 철저한 병렬 처리가 이루어지는 것이다.

## 2.5. 트랜스포메이션, 액션, 지연 평가

트랜스포메이션은 이미 불변성의 특징을 가진 원본 데이터를 수정하지 않고 하나의 스파크 데이터 프레임을 새로운 데이터 프레임으로 그 이름처럼 변형(transform)한다.

### 2.5.1. 좁은/넓은 트랜스포메이션

## 2.6. 스파크 UI

## 2.7. 첫 번째 단독 애플리케이션

### 2.7.1. 쿠키 몬스터를 위한 M&M 세기

### 2.7.2. 스칼라로 만든 단독 애플리케이션 빌드하기

## 2.8. 요약

