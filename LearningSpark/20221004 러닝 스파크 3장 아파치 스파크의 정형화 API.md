[2장](https://velog.io/@qjsmdk1346/Book-%EB%9F%AC%EB%8B%9D-%EC%8A%A4%ED%8C%8C%ED%81%AC-2%EC%9E%A5-%EC%95%84%ED%8C%8C%EC%B9%98-%EC%8A%A4%ED%8C%8C%ED%81%AC-%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C-%EB%B0%8F-%EC%8B%9C%EC%9E%91)에 이어서 공부해보자.

[코드는 여기 있다](https://github.com/Jung0Jin/LearningSpark)

# 3. 아파치 스파크의 정형화 API

이 장에서는 아파치 스파크에 정형화된 구조(Structure)를 추가하게 된 주된 동기와 그 동기가 어떻게 상위 수준 API(데이터 프레임과 데이터세트) 개발로 이어졌는지, 스파크 2.x에서 컴포넌트들 간에 통일성을 확립했는지 등을 둘러본다.

## 3.1. 스파크: RDD의 아래에는 무엇이 있는가

RDD는 스파크에서 가장 기본적인 추상적 부분으로, RDD에는 세 가지의 핵심 특성이 있다.

1. 의존성(Dependency)

2. 파티션(Partition)(지역성 정보 포함)

3. 연산 함수(Compute Function): Partition -> Iterator[T]

의존성: 어떤 입력을 필요로 하고 현재의 RDD가 어떻게 만들어지는지 스파크에게 가르쳐준다. 결과를 새로 만들어야 하는 경우에는 스파크는 이 의존성 정보를 참고하고 연산을 다시 반복해서 RDD를 다시 만들 수 있다.

파티션: 스파크에게 작업을 나눠서 이그제큐터들에 분산해 파티션별로 병렬 연산할 수 있는 능력을 부여한다.

연산 함수: RDD는 RDD에 저장되는 데이터를 Iterator[T] 형태로 만들어 주는 연산 함수를 갖고 있다.

위에 설명한 원조 모델에는 몇 가지 문제가 있었다. 일단 연산 함수나 연산식 자체가 스파크에 투명하지 않았다. 두번째 문제는 Iterator[T] 데이터 타입이 파이썬 RDD에서 불투명했다. 세번째 문제는 스파크가 함수에서의 연산이나 표현식을 검사하지 못하다 보니 최적화할 방법이 없었다. 마지막 네번째 문제는 스파크는 위에서 T로 표시한 타입에 대한 정보가 전혀 없었다.

이런 불투명함은 스파크가 연산 순서를 재정렬해 효과적인 질의 계획으로 바꾸는 능력을 방해했다. 그에 대한 해법으로 아래가 나온다.

## 3.2. 스파크의 구조 확립

스파크 2.x는 스파크 구조 확립을 위한 핵심 개념들을 도입했다.

1. 데이터 분석을 통해 찾은 일상적인 패턴들을 써서 연산을 표현했다.

2. DSL에서 일반적인 연산 집합을 사용함으로써 더 좁혀졌다. 

3. SQL의 테이블이나 스프레드시트처럼, 지원하는 정형화 데이터 타입을 써서 데이터를 표 형태로 구성할 수 있게 되었다.

이런 구조가 왜 좋은지 아래 나온다.

### 3.2.1. 핵심적인 장점과 이득

구조를 갖추면 스파크 컴포넌트를 통틀어 더 나은 성능과 공간 효율성 등 많은 이득을 얻을 수 있다.

예제를 통해 표현성과 구성 용이성을 살펴보자. 

저수준의 RDD API 코드

```
# 저수준의 RDD API 패턴 코드

# 파이썬 예제
from pyspark import SparkContext
sc = SparkContext()

# (name, age) 형태의 튜플로 된 RDD를 생성한다.
dataRDD = sc.parallelize([('Brooke', 20), ('Denny', 31), ('Jules', 30), ('TD', 35), ('Brooke', 25)])

# 집계와 평균을 위한 람다 표현식과 함께 map과 reduceByKey 트랜스포메이션을 사용한다.
agesRDD = (dataRDD.map(lambda x: (x[0] (x[1], 1))).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).map(lambda x: (x[0], x[1][0]/x[1][1])))
```

고수준 DSL 연산자들과 데이터 프레임 API를 쓴 코드

```
# 고수준 DSL 연산자들과 데이터 프레임 API를 쓴 코드

# 파이썬 예제
from pyspark.sql import SparkSession
from pyspark.sql.functions import avg

# SparkSession으로부터 데이터 프레임을 만든다
spark = (SparkSession.builder.appName('AuthorsAges').getOrCreate())

# 데이터 프레임 생성
data_df = spark.createDataFrame([('Brooke', 20), ('Denny', 31), ('Jules', 30), ('TD', 35), ('Brooke', 25)], ['name', 'age'])

# 동일한 이름으로 그룹화하여 나이별로 계산해 평균을 구한다.
avg_df = data_df.groupBy('name').agg(avg('age'))

# 최종 실행 결과를 보여준다.
avg_df.show()
```

개발자들이 중요시하는 단순성이나 표현력은 상위 수준 구조화 API 위에 구축된 스파크 SQL 엔진 덕택에 가능한 것이다.

다음으로 자주 쓰이는 연산들을 위한 API와 DSL이 데이터 분석에서 어떻게 쓰이는지 보자.

## 3.3. 데이터 프레임 API

### 3.3.1. 스파크의 기본 데이터 타입

### 3.3.2. 스파크의 정형화 타입과 복합 타입

### 3.3.3. 스키마와 데이터 프레임 만들기

스파크에서 스키마(Schema)는 데이터 프레임을 위해 칼럼 이름과 연관된 데이터 타입을 정의한 것이다.

스키마를 정희하는 장점

1. 스파크가 데이터 타입을 추축해야 하는 책임을 덜어 준다.

2. 스파크가 스키마를 확정하기 위해 파일의 많은 부분을 읽어 들이려고 별도의 잡을 만드는 것을 방지한다. 데이터 파일이 큰 경우, 이는 비용과 시간이 많이 드는 작업이다.

3. 데이터가 스키마와 맞지 않는 경우, 조기에 문제를 발견할 수 있다.

#### 3.3.3.1. 스키마를 정의하는 두 가지 방법

1. 프로그래밍 스타일로 정의하는 것

2. DDL(Data Definition Language)를 사용하는 것

```
# 세 개의 이름이 붙은 칼럼을 가진 데이터 프레임을 위한 스키마를 프로그래밍 스타일로 정의하려면 스파크 데이터 프레임 API를 사용

# 파이선 예제 
from pyspark.sql.types import *
schema = StructType([StructField('author', StringType(), False),
                    StructField('title', StringType(), False),
                    StructField('pages', IntegerType(), False)])
```

```
# DDL을 써서 동일한 스키마 정의

# 파이썬 예제
schema = 'author STRING, title STRING, pages INT'
```

```
# 어떤 방식이든 원하는 쪽으로 스파크를 정의할 수 있다.
# 이후 예제들에서 양쪽 다 사용하게 될 것이다.

# 파이썬 예제
from pyspark.sql import SparkSession

# DDL을 써서 스키마를 정의한다.
schema = StructType([
   StructField("Id", IntegerType(), False),
   StructField("First", StringType(), False),
   StructField("Last", StringType(), False),
   StructField("Url", StringType(), False),
   StructField("Published", StringType(), False),
   StructField("Hits", IntegerType(), False),
   StructField("Campaigns", ArrayType(StringType()), False)])

# 기본 데이터 생성
data = [[1, "Jules", "Damji", "https://tinyurl.1", "1/4/2016", 4535, ["twitter", "LinkedIn"]],
       [2, "Brooke","Wenig","https://tinyurl.2", "5/5/2018", 8908, ["twitter", "LinkedIn"]],
       [3, "Denny", "Lee", "https://tinyurl.3","6/7/2019",7659, ["web", "twitter", "FB", "LinkedIn"]],
       [4, "Tathagata", "Das","https://tinyurl.4", "5/12/2018", 10568, ["twitter", "FB"]],
       [5, "Matei","Zaharia", "https://tinyurl.5", "5/14/2014", 40578, ["web", "twitter", "FB", "LinkedIn"]],
       [6, "Reynold", "Xin", "https://tinyurl.6", "3/2/2015", 25568, ["twitter", "LinkedIn"]]
      ]

# 메인 프로그램
if __name__ == "__main__":
    # SparkSession 생성
    spark = (SparkSession
       .builder
       .appName("Example-3_6")
       .getOrCreate())
    # 위에 정의했던 스키마로 데이터 프레임 생성
    blogs_df = spark.createDataFrame(data, schema)
   
    # 데이터 프레임 내용을 보여준다. 위에서 만든 데이터를 보여주게 된다.
    #blogs_df.show()
    print()
    
    # 데이터 프레임 처리에 사용된 스키마를 출력한다.
    print(blogs_df.printSchema())
```

### 3.3.4. 칼럼과 표현식

### 3.3.5. 로우

```
# 파이썬 예제
from pyspark.sql import Row

blog_row = Row(6, "Reynold", "Xin", "https://tinyurl.6", "3/2/2015", 25568, ["twitter", "LinkedIn"])
blog_row[1]
```

```
# 파이썬 예제
rows = [Row('Matei Zaharia', 'CA'), Row('Reynold Xin', 'CA')]
authors_df = spark.createDataFrame(rows, ['Authors', 'State'])
authors_df
```

### 3.3.6. 자주 쓰이는 데이터 프레임 작업들

#### 3.3.6.1. DataFrameReader와 DataFrameWriter 사용하기

#### 3.3.6.2. 데이터 프레임을 파케이 파일이나 SQL 테이블로 저장하기

#### 3.3.6.3. 트랜스포메이션과 액션

#### 3.3.6.4. 프로젝션과 필터

#### 3.3.6.5. 칼럼의 이름 변경 및 추가 삭제

#### 3.3.6.6. 집계연산

#### 3.3.6.7. 그 외 일반적인 데이터 프레임 연산들

### 3.3.7. 종단 간 데이터 프레임 예제

지금까지 정형화 API 중 하나인 데이터 프레임 API에 대해 넓게 다루었다. 이는 나중에 스파크 MLlib과 정형화 스트리밍 컴포넌트로 확장된다.

## 3.4. 데이터세트 API

데이터세트는 정적 타입 API(Typed API) 통적 타입 API(Untyped API)의 두 특성을 모두 가진다.

### 3.4.1. 정적 타입 객체, 동적 타입 객체, 포괄적인 Row

### 3.4.2. 데이터세트 생성

#### 3.4.2.1. 스칼라: 케이스 클래스

### 3.4.3. 데이터세트에서 가능한 작업들

### 3.4.4. 시작부터 끝까지 다룬 데이터세트 예제

## 3.5. 데이터 프레임 Vs. 데이터세트

1. 스파크에게 어떻게 하는지가 아니라 무엇을 해야 하는지 말하고 싶으면 데이터 프레임이나 데이터세트를 사용한다.

2. 풍부한 표현과 높은 수준의 추상화 및 DSL 연산을 원한다면 데이터 프레임이나 데이터세트를 사용한다.

3. 컴파일 타임에 엄격한 타입 체크를 원하며 특정한 Dataset[T]를 위해 여러 개의 케이스 클래스를 만드는 것에 부담이 없다면 데이터세트를 사용한다.

4. 자신의 작업이 높은 수준의 표현력, 필터, 맵, 집계, 평균과 합계 계산, SQL 질의, 칼럼 지향 접근, 반정형화된 데이터에 대한 관계형 연산 등이 필요하다면 데이터 프레임이나 데이터세트를 사용한다.

5. 자신의 작업이 SQL과 유사한 질의를 쓰는 관계형 변환을 필요로 한다면 데이터 프레임을 사용한다.

6. 만약 인코더(Encoder)를 써서 프로젝트 텅스텐의 직렬화 능력을 통한 이득을 보고 싶다면 데이터세트를 사용한다.

7. 일원화, 코드 최적화, 스파크 컴포넌트들 사이에서의 API 단순화 등을 원한다면 데이터 프레임을 사용한다.

8. 파이썬 사용자라면 데이터 프레임을 쓰되, 제어권을 좀 더 갖고 싶으면 RDD로 바꿔 사용한다.

9. 공간/속도의 효율성을 원하면 데이터 프레임을 사용한다.

10. 실행 시에 발생하는 에러를 찾기보다 컴파일 시에 발생하는 에러를 찾고 싶다면 적절한 API를 선택하라.

### 3.5.1. 언제 RDD를 사용하는가

## 3.6. 스파크 SQL과 하부의 엔진

### 3.6.1. 카탈리스트 옵티마이저

#### 3.6.1.1. 1단계: 분석

#### 3.6.1.2. 2단계: 논리적 최적화

#### 3.6.1.3. 3단계: 물리 계획 수립

#### 3.6.1.4. 4단계: 코드 생성

## 3.7. 요약

1. 스파크의 정형화 역사와 이점

2. 스파크의 정형화 API

3. 데이터 프레임 및 데이터세트 API가 하위 수준의 RDD API에 비해 훨씬 표현력이 높고 직관적

4. 규모가 큰 데이터를 더 쉽게 처리할 수 있도록 설계된 정형화 API는 일반적인 데이터 작업들을 위해 각 연산에 필수적인 함수를 제공함

5. 스파크 SQL 엔진의 주 컴포넌트인 카탈리스트 옵티마이저와 프로젝트 텅스텐이 상위 수준의 정형화 API와 DSL 함수들을 어떻게 지원하는지 내부를 들여다보았다.
